<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quantum Circuit Born Machine · Yao.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-89508993-1', 'auto');
ga('send', 'pageview');
</script><link rel="canonical" href="https://quantumbfs.github.io/Yao.jl/latest/tutorial/QCBM/index.html"/><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="Yao.jl logo"/></a><h1>Yao.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><span class="toctext">Tutorial</span><ul><li><a class="toctext" href="../RegisterBasics/">Register Basics</a></li><li><a class="toctext" href="../BlockBasics/">Block Basics</a></li><li><a class="toctext" href="../Diff/">Automatic Differentiation</a></li><li><a class="toctext" href="../BinaryBasics/">Binary Basics</a></li></ul></li><li><span class="toctext">Examples</span><ul><li><a class="toctext" href="../GHZ/">Prepare Greenberger–Horne–Zeilinger state with Quantum Circuit</a></li><li><a class="toctext" href="../QFT/">Quantum Fourier Transformation and Phase Estimation</a></li><li><a class="toctext" href="../Grover/">Grover Search and Quantum Inference</a></li><li class="current"><a class="toctext" href>Quantum Circuit Born Machine</a><ul class="internal"><li><a class="toctext" href="#Training-target-1">Training target</a></li><li><a class="toctext" href="#Build-Circuits-1">Build Circuits</a></li><li><a class="toctext" href="#MMD-Loss-and-Gradients-1">MMD Loss &amp; Gradients</a></li><li><a class="toctext" href="#Optimizer-1">Optimizer</a></li><li><a class="toctext" href="#Start-Training-1">Start Training</a></li></ul></li></ul></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="../../man/yao/">Yao</a></li><li><a class="toctext" href="../../man/interfaces/">Interfaces</a></li><li><a class="toctext" href="../../man/registers/">Registers</a></li><li><a class="toctext" href="../../man/blocks/">Blocks System</a></li><li><a class="toctext" href="../../man/intrinsics/">Intrinsics</a></li><li><a class="toctext" href="../../man/boost/">Boost</a></li></ul></li><li><span class="toctext">Developer Documentation</span><ul><li><a class="toctext" href="../../dev/extending-blocks/">Extending Blocks</a></li><li><a class="toctext" href="../../dev/benchmark/">Benchmark with ProjectQ</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Examples</li><li><a href>Quantum Circuit Born Machine</a></li></ul><a class="edit-page" href="https://github.com/QuantumBFS/Yao.jl/blob/master/../../../../build/QuantumBFS/Yao.jl/docs/src/tutorial/QCBM.jl"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Quantum Circuit Born Machine</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Quantum-Circuit-Born-Machine-1" href="#Quantum-Circuit-Born-Machine-1">Quantum Circuit Born Machine</a></h1><p>Reference: <a href="https://arxiv.org/abs/1804.04168">Jin-Guo Liu, Lei Wang (2018)</a> Differentiable Learning of Quantum Circuit Born Machine</p><div><pre><code class="language-julia">using Yao, Yao.Blocks
using LinearAlgebra</code></pre></div><h2><a class="nav-anchor" id="Training-target-1" href="#Training-target-1">Training target</a></h2><p>A gaussian distribution</p><div>\[f(x \left| \mu, \sigma^2\right) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</div><div><pre><code class="language-julia">function gaussian_pdf(x, μ::Real, σ::Real)
    pl = @. 1 / sqrt(2pi * σ^2) * exp(-(x - μ)^2 / (2 * σ^2))
    pl / sum(pl)
end
pg = gaussian_pdf(1:1&lt;&lt;6, 1&lt;&lt;5-0.5, 1&lt;&lt;4);</code></pre></div><p>This distribution looks like <img src="../../assets/figures/QCBM_1.svg" alt="Gaussian Distribution"/></p><h2><a class="nav-anchor" id="Build-Circuits-1" href="#Build-Circuits-1">Build Circuits</a></h2><h3><a class="nav-anchor" id="Building-Blocks-1" href="#Building-Blocks-1">Building Blocks</a></h3><p>Gates are grouped to become a layer in a circuit, this layer can be <strong>Arbitrary Rotation</strong> or <strong>CNOT entangler</strong>. Which are used as our basic building blocks of <strong>Born Machines</strong>.</p><p><img src="../../assets/figures/differentiable.png" alt="differentiable ciruit"/></p><h4><a class="nav-anchor" id="Arbitrary-Rotation-1" href="#Arbitrary-Rotation-1">Arbitrary Rotation</a></h4><p>Arbitrary Rotation is built with <strong>Rotation Gate on Z</strong>, <strong>Rotation Gate on X</strong> and <strong>Rotation Gate on Z</strong>:</p><div>\[Rz(\theta) \cdot Rx(\theta) \cdot Rz(\theta)\]</div><p>Since our input will be a <span>$|0\dots 0\rangle$</span> state. The first layer of arbitrary rotation can just use <span>$Rx(\theta) \cdot Rz(\theta)$</span> and the last layer of arbitrary rotation could just use <span>$Rz(\theta)\cdot Rx(\theta)$</span></p><p>In <strong>幺</strong>, every Hilbert operator is a <strong>block</strong> type, this includes all <strong>quantum gates</strong> and <strong>quantum oracles</strong>. In general, operators appears in a quantum circuit can be divided into <strong>Composite Blocks</strong> and <strong>Primitive Blocks</strong>.</p><p>We follow the low abstraction principle and thus each block represents a certain approach of calculation. The simplest <strong>Composite Block</strong> is a <strong>Chain Block</strong>, which chains other blocks (oracles) with the same number of qubits together. It is just a simple mathematical composition of operators with same size. e.g.</p><div>\[\text{chain(X, Y, Z)} \iff X \cdot Y \cdot Z\]</div><p>We can construct an arbitrary rotation block by chain <span>$Rz$</span>, <span>$Rx$</span>, <span>$Rz$</span> together.</p><div><pre><code class="language-julia">chain(Rz(0), Rx(0), Rz(0))</code></pre><pre><code class="language-none">Total: 1, DataType: Complex{Float64}
chain
├─ Rot Z gate: 0.0
├─ Rot X gate: 0.0
└─ Rot Z gate: 0.0</code></pre></div><p><code>Rx</code>, <code>Ry</code> and <code>Rz</code> will construct new rotation gate, which are just shorthands for <code>rot(X, 0.0)</code>, etc.</p><p>Then, let&#39;s chain them up</p><div><pre><code class="language-julia">layer(nbit::Int, x::Symbol) = layer(nbit, Val(x))
layer(nbit::Int, ::Val{:first}) = chain(nbit, put(i=&gt;chain(Rx(0), Rz(0))) for i = 1:nbit);</code></pre></div><p>Here, we do not need to feed the first <code>nbit</code> parameter into <code>put</code>. All factory methods can be <strong>lazy</strong> evaluate <strong>the first arguements</strong>, which is the number of qubits. It will return a lambda function that requires a single interger input. The instance of desired block will only be constructed until all the information is filled. When you filled all the information in somewhere of the declaration, 幺 will be able to infer the others. We will now define the rest of rotation layers</p><div><pre><code class="language-julia">layer(nbit::Int, ::Val{:last}) = chain(nbit, put(i=&gt;chain(Rz(0), Rx(0))) for i = 1:nbit)
layer(nbit::Int, ::Val{:mid}) = chain(nbit, put(i=&gt;chain(Rz(0), Rx(0), Rz(0))) for i = 1:nbit);</code></pre></div><h4><a class="nav-anchor" id="CNOT-Entangler-1" href="#CNOT-Entangler-1">CNOT Entangler</a></h4><p>Another component of quantum circuit born machine is several <strong>CNOT</strong> operators applied on different qubits.</p><div><pre><code class="language-julia">entangler(pairs) = chain(control([ctrl, ], target=&gt;X) for (ctrl, target) in pairs);</code></pre></div><p>We can then define such a born machine</p><div><pre><code class="language-julia">function build_circuit(n::Int, nlayer::Int, pairs)
    circuit = chain(n)
    push!(circuit, layer(n, :first))

    for i = 1:(nlayer - 1)
        push!(circuit, cache(entangler(pairs)))
        push!(circuit, layer(n, :mid))
    end

    push!(circuit, cache(entangler(pairs)))
    push!(circuit, layer(n, :last))

    circuit
end;</code></pre></div><p>We use the method <code>cache</code> here to tag the entangler block that it should be cached after its first run, because it is actually a constant oracle. Let&#39;s see what will be constructed</p><pre><code class="language-julia-repl">julia&gt; build_circuit(4, 1, [1=&gt;2, 2=&gt;3, 3=&gt;4]) |&gt; autodiff(:QC)
Total: 4, DataType: Complex{Float64}
chain
├─ chain
│  ├─ put on (1)
│  │  └─ chain
│  │     ├─ [̂∂] Rot X gate: 0.0
│  │     └─ [̂∂] Rot Z gate: 0.0
│  ├─ put on (2)
│  │  └─ chain
│  │     ├─ [̂∂] Rot X gate: 0.0
│  │     └─ [̂∂] Rot Z gate: 0.0
│  ├─ put on (3)
│  │  └─ chain
│  │     ├─ [̂∂] Rot X gate: 0.0
│  │     └─ [̂∂] Rot Z gate: 0.0
│  └─ put on (4)
│     └─ chain
│        ├─ [̂∂] Rot X gate: 0.0
│        └─ [̂∂] Rot Z gate: 0.0
├─ [↺] chain
│  ├─ control(1)
│  │  └─ (2,)=&gt;X gate
│  ├─ control(2)
│  │  └─ (3,)=&gt;X gate
│  └─ control(3)
│     └─ (4,)=&gt;X gate
└─ chain
   ├─ put on (1)
   │  └─ chain
   │     ├─ [̂∂] Rot Z gate: 0.0
   │     └─ [̂∂] Rot X gate: 0.0
   ├─ put on (2)
   │  └─ chain
   │     ├─ [̂∂] Rot Z gate: 0.0
   │     └─ [̂∂] Rot X gate: 0.0
   ├─ put on (3)
   │  └─ chain
   │     ├─ [̂∂] Rot Z gate: 0.0
   │     └─ [̂∂] Rot X gate: 0.0
   └─ put on (4)
      └─ chain
         ├─ [̂∂] Rot Z gate: 0.0
         └─ [̂∂] Rot X gate: 0.0</code></pre><p><a href="../../man/blocks/#Yao.Blocks.RotationGate"><code>RotationGate</code></a>s inside this circuit are automatically marked by [̂∂], which means parameters inside are diferentiable. <code>autodiff</code> has two modes, one is <code>autodiff(:QC)</code>, which means quantum differentiation with simulation complexity <span>$O(M^2)$</span> (<span>$M$</span> is the number of parameters), the other is classical backpropagation <code>autodiff(:BP)</code> with simulation coplexity <span>$O(M)$</span>.</p><p>Let&#39;s define a circuit to use later</p><div><pre><code class="language-julia">circuit = build_circuit(6, 10, [1=&gt;2, 3=&gt;4, 5=&gt;6, 2=&gt;3, 4=&gt;5, 6=&gt;1]) |&gt; autodiff(:QC)
dispatch!(circuit, :random);</code></pre></div><p>Here, the function <code>autodiff(:QC)</code> will mark rotation gates in a circuit as differentiable automatically.</p><h2><a class="nav-anchor" id="MMD-Loss-and-Gradients-1" href="#MMD-Loss-and-Gradients-1">MMD Loss &amp; Gradients</a></h2><p>The MMD loss is describe below:</p><div>\[\begin{aligned}
\mathcal{L} &amp;= \left| \sum_{x} p \theta(x) \phi(x) - \sum_{x} \pi(x) \phi(x) \right|^2\\
            &amp;= \langle K(x, y) \rangle_{x \sim p_{\theta}, y\sim p_{\theta}} - 2 \langle K(x, y) \rangle_{x\sim p_{\theta}, y\sim \pi} + \langle K(x, y) \rangle_{x\sim\pi, y\sim\pi}
\end{aligned}\]</div><p>We will use a squared exponential kernel here.</p><div><pre><code class="language-julia">struct RBFKernel
    sigma::Float64
    matrix::Matrix{Float64}
end

&quot;&quot;&quot;get kernel matrix&quot;&quot;&quot;
kmat(mbf::RBFKernel) = mbf.matrix

&quot;&quot;&quot;statistic functional for kernel matrix&quot;&quot;&quot;
kernel_expect(kernel::RBFKernel, px::Vector, py::Vector=px) = px&#39; * kmat(kernel) * py;</code></pre></div><p>Now let&#39;s define the RBF kernel matrix used in calculation</p><div><pre><code class="language-julia">function rbf_kernel(basis, σ::Real)
    dx2 = (basis .- basis&#39;).^2
    RBFKernel(σ, exp.(-1/2σ * dx2))
end

kernel = rbf_kernel(0:1&lt;&lt;6-1, 0.25);</code></pre></div><p>Next, we build a QCBM setup, which is a combination of <code>circuit</code>, <code>kernel</code> and target probability distribution <code>ptrain</code> Its loss function is MMD loss, if and only if it is 0, the output probability of circuit matches <code>ptrain</code> exactly.</p><div><pre><code class="language-julia">struct QCBM{BT&lt;:AbstractBlock}
    circuit::BT
    kernel::RBFKernel
    ptrain::Vector{Float64}
end

&quot;&quot;&quot;get wave function&quot;&quot;&quot;
psi(qcbm::QCBM) = zero_state(qcbm.circuit |&gt; nqubits) |&gt; qcbm.circuit

&quot;&quot;&quot;extract probability dierctly&quot;&quot;&quot;
Yao.probs(qcbm::QCBM) = qcbm |&gt; psi |&gt; probs

&quot;&quot;&quot;the loss function&quot;&quot;&quot;
function mmd_loss(qcbm, p=qcbm|&gt;probs)
    p = p - qcbm.ptrain
    kernel_expect(qcbm.kernel, p, p)
end;</code></pre></div><p>problem setup</p><div><pre><code class="language-julia">qcbm = QCBM(circuit, kernel, pg);</code></pre></div><h3><a class="nav-anchor" id="Gradients-1" href="#Gradients-1">Gradients</a></h3><p>the gradient of MMD loss is</p><div>\[\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \theta^i_l} &amp;= \langle K(x, y) \rangle_{x\sim p_{\theta^+}, y\sim p_{\theta}} - \langle K(x, y) \rangle_{x\sim p_{\theta}^-, y\sim p_{\theta}}\\
&amp;- \langle K(x, y) \rangle _{x\sim p_{\theta^+}, y\sim\pi} + \langle K(x, y) \rangle_{x\sim p_{\theta^-}, y\sim\pi}
\end{aligned}\]</div><div><pre><code class="language-julia">function mmdgrad(qcbm::QCBM, dbs; p0::Vector)
    statdiff(()-&gt;probs(qcbm) |&gt; as_weights, dbs, StatFunctional(kmat(qcbm.kernel)), initial=p0 |&gt; as_weights) -
        2*statdiff(()-&gt;probs(qcbm) |&gt; as_weights, dbs, StatFunctional(kmat(qcbm.kernel)*qcbm.ptrain))
end;</code></pre></div><h2><a class="nav-anchor" id="Optimizer-1" href="#Optimizer-1">Optimizer</a></h2><p>We will use the Adam optimizer. Since we don&#39;t want you to install another package for this, the following code for this optimizer is copied from <a href="https://github.com/denizyuret/Knet.jl">Knet.jl</a></p><p>Reference: <a href="https://arxiv.org/abs/1412.6980">Kingma, D. P., &amp; Ba, J. L. (2015)</a>. Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13.</p><div><pre><code class="language-julia">mutable struct Adam
    lr::AbstractFloat
    gclip::AbstractFloat
    beta1::AbstractFloat
    beta2::AbstractFloat
    eps::AbstractFloat
    t::Int
    fstm
    scndm
end

Adam(; lr=0.001, gclip=0, beta1=0.9, beta2=0.999, eps=1e-8)=Adam(lr, gclip, beta1, beta2, eps, 0, nothing, nothing)

function update!(w, g, p::Adam)
    gclip!(g, p.gclip)
    if p.fstm===nothing; p.fstm=zero(w); p.scndm=zero(w); end
    p.t += 1
    lmul!(p.beta1, p.fstm)
    BLAS.axpy!(1-p.beta1, g, p.fstm)
    lmul!(p.beta2, p.scndm)
    BLAS.axpy!(1-p.beta2, g .* g, p.scndm)
    fstm_corrected = p.fstm / (1 - p.beta1 ^ p.t)
    scndm_corrected = p.scndm / (1 - p.beta2 ^ p.t)
    BLAS.axpy!(-p.lr, @.(fstm_corrected / (sqrt(scndm_corrected) + p.eps)), w)
end

function gclip!(g, gclip)
    if gclip == 0
        g
    else
        gnorm = vecnorm(g)
        if gnorm &lt;= gclip
            g
        else
            BLAS.scale!(gclip/gnorm, g)
        end
    end
end
optim = Adam(lr=0.1);</code></pre></div><h2><a class="nav-anchor" id="Start-Training-1" href="#Start-Training-1">Start Training</a></h2><p>We define an iterator called <code>QCBMOptimizer</code>. We want to realize some interface like</p><pre><code class="language-julia">for x in qo
    # runtime result analysis
end</code></pre><p>Although such design makes the code a bit more complicated, but one will benefit from this interfaces when doing run time analysis, like keeping track of the loss.</p><div><pre><code class="language-julia">struct QCBMOptimizer
    qcbm::QCBM
    optimizer
    dbs
    params::Vector
    QCBMOptimizer(qcbm::QCBM, optimizer) = new(qcbm, optimizer, collect(qcbm.circuit, AbstractDiff), parameters(qcbm.circuit))
end</code></pre></div><p>In the initialization of <code>QCBMOptimizer</code> instance, we collect all differentiable units into a sequence <code>dbs</code> for furture use.</p><p><strong>iterator interface</strong> To support iteration operations, <a href="https://docs.julialang.org/en/v1/manual/interfaces/index.html#man-interface-iteration-1"><code>Base.iterate</code></a> should be implemented</p><div><pre><code class="language-julia">function Base.iterate(qo::QCBMOptimizer, state::Int=1)
    p0 = qo.qcbm |&gt; probs
    grad = mmdgrad.(Ref(qo.qcbm), qo.dbs, p0=p0)
    update!(qo.params, grad, qo.optimizer)
    dispatch!(qo.qcbm.circuit, qo.params)
    (p0, state+1)
end</code></pre></div><p>In each iteration, the iterator will return the generated probability distribution in current step. During each iteration step, we broadcast <code>mmdgrad</code> function over <code>dbs</code> to obtain all gradients. Here, To avoid the QCBM instance from being broadcasted, we wrap it with <a href="https://docs.julialang.org/en/v1/base/c/#Core.Ref"><code>Ref</code></a> to create a reference for it. The training of the quantum circuit is simple, just iterate through the steps.</p><div><pre><code class="language-julia">history = Float64[]
for (k, p) in enumerate(QCBMOptimizer(qcbm, optim))
    curr_loss = mmd_loss(qcbm, p)
    push!(history, curr_loss)
    k%5 == 0 &amp;&amp; println(&quot;k = &quot;, k, &quot; loss = &quot;, curr_loss)
    k &gt;= 50 &amp;&amp; break
end</code></pre><pre><code class="language-none">k = 5 loss = 0.007208280907173909
k = 10 loss = 0.0032614619464822027
k = 15 loss = 0.0018071680504468126
k = 20 loss = 0.00092582482672117
k = 25 loss = 0.0005869910478368788
k = 30 loss = 0.00032009421045907295
k = 35 loss = 0.00021498938164549723
k = 40 loss = 0.00015115298928988836
k = 45 loss = 7.919052433584494e-5
k = 50 loss = 3.887272654018103e-5</code></pre></div><p>The training history looks like <img src="../../assets/figures/QCBM_2.svg" alt="History"/></p><p>and the learnt distribution <img src="../../assets/figures/QCBM_3.svg" alt="Learnt Distribution"/></p><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><footer><hr/><a class="previous" href="../Grover/"><span class="direction">Previous</span><span class="title">Grover Search and Quantum Inference</span></a><a class="next" href="../../man/yao/"><span class="direction">Next</span><span class="title">Yao</span></a></footer></article></body></html>
